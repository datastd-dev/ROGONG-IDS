{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229a362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from imblearn import under_sampling, over_sampling, combine\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c99ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
    "\n",
    "# Assigning attribute name to dataset\n",
    "dataset_train = pd.read_csv(\"*/NSL-KDD/KDDTrain+.csv\", header=None,names=col_names,sep=\";\")\n",
    "dataset_test = pd.read_csv(\"*/NSL-KDD/KDDTest+.csv\", header=None, names=col_names,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9bddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution Training set:\n",
      "normal             67343\n",
      "neptune            41214\n",
      "satan               3633\n",
      "ipsweep             3599\n",
      "portsweep           2931\n",
      "smurf               2646\n",
      "nmap                1493\n",
      "back                 956\n",
      "teardrop             892\n",
      "warezclient          890\n",
      "pod                  201\n",
      "guess_passwd          53\n",
      "buffer_overflow       30\n",
      "warezmaster           20\n",
      "land                  18\n",
      "imap                  11\n",
      "rootkit               10\n",
      "loadmodule             9\n",
      "ftp_write              8\n",
      "multihop               7\n",
      "phf                    4\n",
      "perl                   3\n",
      "spy                    2\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Label distribution Test set:\n",
      "normal             9711\n",
      "neptune            4657\n",
      "guess_passwd       1231\n",
      "mscan               996\n",
      "warezmaster         944\n",
      "apache2             737\n",
      "satan               735\n",
      "processtable        685\n",
      "smurf               665\n",
      "back                359\n",
      "snmpguess           331\n",
      "saint               319\n",
      "mailbomb            293\n",
      "snmpgetattack       178\n",
      "portsweep           157\n",
      "ipsweep             141\n",
      "httptunnel          133\n",
      "nmap                 73\n",
      "pod                  41\n",
      "buffer_overflow      20\n",
      "multihop             18\n",
      "named                17\n",
      "ps                   15\n",
      "sendmail             14\n",
      "xterm                13\n",
      "rootkit              13\n",
      "teardrop             12\n",
      "xlock                 9\n",
      "land                  7\n",
      "xsnoop                4\n",
      "ftp_write             3\n",
      "sqlattack             2\n",
      "perl                  2\n",
      "udpstorm              2\n",
      "worm                  2\n",
      "phf                   2\n",
      "loadmodule            2\n",
      "imap                  1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#label distribution of Training set and testing set\n",
    "print('Label distribution Training set:')\n",
    "print(dataset_train['label'].value_counts())\n",
    "print()\n",
    "print('Label distribution Test set:')\n",
    "print(dataset_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1614e234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 70 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 23 categories\n",
      "\n",
      "Distribution of categories in service:\n",
      "http        40338\n",
      "private     21853\n",
      "domain_u     9043\n",
      "smtp         7313\n",
      "ftp_data     6860\n",
      "Name: service, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n",
    "# explore categorical features\n",
    "print('Training set:')\n",
    "for col_name in dataset_train.columns:\n",
    "    if dataset_train[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(dataset_train[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "#see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\n",
    "print()\n",
    "print('Distribution of categories in service:')\n",
    "print(dataset_train['service'].value_counts().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774faf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 64 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 38 categories\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "print('Test set:')\n",
    "for col_name in dataset_test.columns:\n",
    "    if dataset_test[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(dataset_test[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4eaac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=['protocol_type', 'service', 'flag']\n",
    "# insert code to get a list of categorical columns into a variable, categorical_columns\n",
    "categorical_columns=['protocol_type', 'service', 'flag'] \n",
    " # Get the categorical values into a 2D numpy array\n",
    "dataset_train_categorical_values = dataset_train[categorical_columns]\n",
    "dataset_test_categorical_values = dataset_test[categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702614d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protocol_type_icmp', 'Protocol_type_tcp', 'Protocol_type_udp', 'service_IRC', 'service_X11', 'service_Z39_50', 'service_aol', 'service_auth', 'service_bgp', 'service_courier', 'service_csnet_ns', 'service_ctf', 'service_daytime', 'service_discard', 'service_domain', 'service_domain_u', 'service_echo', 'service_eco_i', 'service_ecr_i', 'service_efs', 'service_exec', 'service_finger', 'service_ftp', 'service_ftp_data', 'service_gopher', 'service_harvest', 'service_hostnames', 'service_http', 'service_http_2784', 'service_http_443', 'service_http_8001', 'service_imap4', 'service_iso_tsap', 'service_klogin', 'service_kshell', 'service_ldap', 'service_link', 'service_login', 'service_mtp', 'service_name', 'service_netbios_dgm', 'service_netbios_ns', 'service_netbios_ssn', 'service_netstat', 'service_nnsp', 'service_nntp', 'service_ntp_u', 'service_other', 'service_pm_dump', 'service_pop_2', 'service_pop_3', 'service_printer', 'service_private', 'service_red_i', 'service_remote_job', 'service_rje', 'service_shell', 'service_smtp', 'service_sql_net', 'service_ssh', 'service_sunrpc', 'service_supdup', 'service_systat', 'service_telnet', 'service_tftp_u', 'service_tim_i', 'service_time', 'service_urh_i', 'service_urp_i', 'service_uucp', 'service_uucp_path', 'service_vmnet', 'service_whois', 'flag_OTH', 'flag_REJ', 'flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0', 'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH']\n"
     ]
    }
   ],
   "source": [
    "# protocol type\n",
    "unique_protocol=sorted(dataset_train.protocol_type.unique())\n",
    "string1 = 'Protocol_type_'\n",
    "unique_protocol2=[string1 + x for x in unique_protocol]\n",
    "# service\n",
    "unique_service=sorted(dataset_train.service.unique())\n",
    "string2 = 'service_'\n",
    "unique_service2=[string2 + x for x in unique_service]\n",
    "# flag\n",
    "unique_flag=sorted(dataset_train.flag.unique())\n",
    "string3 = 'flag_'\n",
    "unique_flag2=[string3 + x for x in unique_flag]\n",
    "# put together\n",
    "dumcols=unique_protocol2 + unique_service2 + unique_flag2\n",
    "print(dumcols)\n",
    "\n",
    "#do same for test set\n",
    "unique_service_test=sorted(dataset_test.service.unique())\n",
    "unique_service2_test=[string2 + x for x in unique_service_test]\n",
    "testdumcols=unique_protocol2 + unique_service2_test + unique_flag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0af93cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   protocol_type  service  flag\n",
      "0              1       20     9\n",
      "1              2       44     9\n",
      "2              1       49     5\n",
      "3              1       24     9\n",
      "4              1       24     9\n"
     ]
    }
   ],
   "source": [
    "#Transform categorical features into numbers using LabelEncoder()\n",
    "dataset_train_categorical_values_enc=dataset_train_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "print(dataset_train_categorical_values_enc.head())\n",
    "# test set\n",
    "dataset_test_categorical_values_enc=dataset_test_categorical_values.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5abbd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "dataset_train_categorical_values_encenc = enc.fit_transform(dataset_train_categorical_values_enc)\n",
    "dataset_train_cat_data = pd.DataFrame(dataset_train_categorical_values_encenc.toarray(),columns=dumcols)\n",
    "# test set\n",
    "dataset_test_categorical_values_encenc = enc.fit_transform(dataset_test_categorical_values_enc)\n",
    "dataset_test_cat_data = pd.DataFrame(dataset_test_categorical_values_encenc.toarray(),columns=testdumcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f31564a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['service_red_i',\n",
       " 'service_http_8001',\n",
       " 'service_urh_i',\n",
       " 'service_harvest',\n",
       " 'service_aol',\n",
       " 'service_http_2784']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainservice=dataset_train['service'].tolist()\n",
    "testservice= dataset_test['service'].tolist()\n",
    "difference=list(set(trainservice) - set(testservice))\n",
    "string = 'service_'\n",
    "difference=[string + x for x in difference]\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df23ab17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 84)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in difference:\n",
    "    dataset_test_cat_data[col] = 0\n",
    "\n",
    "dataset_test_cat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a14e82ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 123)\n",
      "(22544, 123)\n"
     ]
    }
   ],
   "source": [
    "#Join encoded categorical dataframe with the non-categorical dataframe\n",
    "newdf=dataset_train.join(dataset_train_cat_data)\n",
    "newdf.drop('flag', axis=1, inplace=True)\n",
    "newdf.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf.drop('service', axis=1, inplace=True)\n",
    "# test data\n",
    "newdf_test=dataset_test.join(dataset_test_cat_data)\n",
    "newdf_test.drop('flag', axis=1, inplace=True)\n",
    "newdf_test.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf_test.drop('service', axis=1, inplace=True)\n",
    "print(newdf.shape)\n",
    "print(newdf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829083e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# take label column\n",
    "labeldf=newdf['label']\n",
    "labeldf_test=newdf_test['label']\n",
    "# change the label column\n",
    "newlabeldf=labeldf.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "newlabeldf_test=labeldf_test.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "# put the new label column back\n",
    "newdf['label'] = newlabeldf\n",
    "newdf_test['label'] = newlabeldf_test\n",
    "print(newdf['label'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e956f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 67343), (1, 45927), (2, 11656), (3, 995), (4, 52)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(newdf.label).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882a9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = newdf.drop(['label'],axis=1).values\n",
    "y = newdf.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47e9f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=40,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27f3bae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 47140), (1, 32149), (2, 8159), (3, 697), (4, 36)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((88181, 122), (88181,), (37792, 122), (37792,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sorted(Counter(y_train).items()))\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f797f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 47140), (1, 32149), (2, 13468), (3, 13468), (4, 13468)]\n"
     ]
    }
   ],
   "source": [
    "a = 13468#5387 \n",
    "smo = SMOTE(sampling_strategy={2:a,3:a,4:a},random_state=42) \n",
    "X_train, y_train = smo.fit_resample(X_train, y_train)   \n",
    "\n",
    "print(sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbb7c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# define the undersampling method\n",
    "undersample = NearMiss(version=1, n_neighbors=3, sampling_strategy={0:a,1:a})\n",
    "X_train, y_train = undersample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7d82511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 13468), (1, 13468), (2, 13468), (3, 13468), (4, 13468)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5753d12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahmetokanarik/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:31:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Train time: 48.412922859191895\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "time_end = time.time()\n",
    "train_time = time_end - time_start\n",
    "print(\"Train time:\",train_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60e9dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test time: 0.1902298927307129\n",
      "Accuracy: 94.31%\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "y_pred = model.predict(X_test) \n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions) \n",
    "time_end = time.time()\n",
    "test_time = time_end - time_start\n",
    "print(\"Test time:\",test_time)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ab85de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.95      0.96      0.95     20203\n",
      "         DoS       1.00      0.91      0.95     13778\n",
      "       Probe       0.98      1.00      0.99      3497\n",
      "         R2L       0.24      0.97      0.39       298\n",
      "         U2R       0.06      0.88      0.10        16\n",
      "\n",
      "    accuracy                           0.94     37792\n",
      "   macro avg       0.65      0.94      0.68     37792\n",
      "weighted avg       0.97      0.94      0.95     37792\n",
      "\n",
      "acc: 94.31096528365792\n",
      "pre: 96.67553188927116\n",
      "DR=recall: 94.31096528365792\n",
      "f1: 95.23144190658651\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['Normal','DoS','Probe','R2L','U2R']\n",
    "acc = metrics.accuracy_score(y_test,predictions) * 100\n",
    "f1 = metrics.f1_score(y_test, predictions,average='weighted')* 100\n",
    "pre = metrics.precision_score(y_test, predictions, labels=None, pos_label=1, average='weighted') * 100 #DR\n",
    "recall = metrics.recall_score(y_test, predictions, labels=None, pos_label=1, average='weighted', sample_weight=None) * 100\n",
    "\n",
    "print(classification_report(y_test,predictions,target_names=target_names))\n",
    "print(\"acc:\",acc)\n",
    "print(\"pre:\",pre)\n",
    "print(\"DR=recall:\",recall)\n",
    "print(\"f1:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df27e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00023892959541255176\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "TP = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "\n",
    "FAR = FP / (FP + TN)\n",
    "print(FAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "307ec8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbayesian optimization\\nimport hyperopt\\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\\nspace={\\'max_depth\\': hp.quniform(\"max_depth\", 3, 18, 1),\\n        \\'gamma\\': hp.uniform (\\'gamma\\', 1,9),\\n        \\'reg_alpha\\' : hp.quniform(\\'reg_alpha\\', 40,180,1),\\n        \\'reg_lambda\\' : hp.uniform(\\'reg_lambda\\', 0,1),\\n        \\'colsample_bytree\\' : hp.uniform(\\'colsample_bytree\\', 0.5,1),\\n        \\'min_child_weight\\' : hp.quniform(\\'min_child_weight\\', 0, 10, 1),\\n        \\'n_estimators\\': 180\\n    }\\n\\ndef hyperparameter_tuning(space):\\n    model = xgb.XGBClassifier(n_estimators =space[\\'n_estimators\\'], max_depth = int(space[\\'max_depth\\']), gamma = space[\\'gamma\\'],\\n                         reg_alpha = int(space[\\'reg_alpha\\']),min_child_weight=space[\\'min_child_weight\\'],\\n                         colsample_bytree=space[\\'colsample_bytree\\'])\\n    evaluation = [( X_train, y_train), ( X_test, y_test)]\\n    \\n    model.fit(X_train, y_train,\\n            eval_set=evaluation, eval_metric=\"\",\\n            early_stopping_rounds=10,verbose=False)\\n\\n    pred = model.predict(X_test)\\n    accuracy = accuracy_score(y_test, pred>0.5)\\n    print (\"SCORE:\", accuracy)\\n    #change the metric if you like\\n    return {\\'loss\\': -accuracy, \\'status\\': STATUS_OK, \\'model\\': model}\\n\\ntrials = Trials()\\nbest = fmin(fn=hyperparameter_tuning,\\n            space=space,\\n            algo=tpe.suggest,\\n            max_evals=30,\\n            trials=trials)\\n\\nprint (best)\\n\\ny_pred = model.predict(X_test) \\npredictions = [round(value) for value in y_pred]\\naccuracy = accuracy_score(y_test, predictions) \\n\\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\\n\\n#Result 91.91%\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bayesian optimization\n",
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 180\n",
    "    }\n",
    "\n",
    "def hyperparameter_tuning(space):\n",
    "    model = xgb.XGBClassifier(n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n",
    "                         reg_alpha = int(space['reg_alpha']),min_child_weight=space['min_child_weight'],\n",
    "                         colsample_bytree=space['colsample_bytree'])\n",
    "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "            eval_set=evaluation, eval_metric=\"\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred>0.5)\n",
    "    print (\"SCORE:\", accuracy)\n",
    "    #change the metric if you like\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=hyperparameter_tuning,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=30,\n",
    "            trials=trials)\n",
    "\n",
    "print (best)\n",
    "\n",
    "y_pred = model.predict(X_test) \n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions) \n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "#Result 91.91%\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
